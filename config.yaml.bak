# Smart Agent Configuration

# LLM API Configuration
llm:
  base_url: "http://grai.gilead.com:4001"            # Base URL for LLM API (local LiteLLM proxy)
  model: "claude-3-7-sonnet-20250219"          # Model to use for generation
  api_key: "api_key"                           # API key for local LiteLLM proxy
  temperature: 1.0                             # Temperature for generation (0.0-1.0)

# Logging Configuration
logging:
  level: "WARNING"                             # Logging level (DEBUG, INFO, WARNING, ERROR)
  file: null                                   # Set to a path to log to a file

# Monitoring Configuration
# monitoring:
#   langfuse:
#     enabled: false                           # Set to true to enable Langfuse monitoring
#     host: "https://cloud.langfuse.com"       # Langfuse host URL
#     public_key: ""                           # Your Langfuse public key
#     secret_key: ""                           # Your Langfuse secret key

# Tools Configuration
tools:
  think:
    enabled: true
    url: http://grai-dev.gilead.com:8000/sse
    command: "uvx --from git+https://github.com/ddkang1/mcp-think mcp-think"
    transport: sse
  ddg_mcp:
    enabled: true
    url: http://grai-dev.gilead.com:8001/sse
    command: "uvx --from git+https://github.com/ddkang1/ddg-mcp ddg-mcp"
    transport: sse
  python_repl:
    enabled: true
    url: http://grai-dev.gilead.com:8002/sse
    command: "docker run -i --rm --pull=always -v ./data:/mnt/data/ ghcr.io/ddkang1/mcp-py-repl:latest"
    transport: stdio_to_sse
  pubmed:
    enabled: true
    url: http://grai-dev.gilead.com:8003/sse
    command: "uvx --from git+https://github.com/ddkang1/mcp-pubmed mcp-pubmed"
    transport: sse