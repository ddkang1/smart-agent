# Smart Agent Configuration Example

# LLM API Configuration
llm:
  base_url: "http://localhost:4000"            # Base URL for LLM API (local LiteLLM proxy)
  model: "claude-3-7-sonnet-20240229"          # Model to use for generation
  api_key: "your-api-key"                      # API key for local LiteLLM proxy
  temperature: 0.7                             # Temperature for generation (0.0-1.0)

# Logging Configuration
logging:
  level: "INFO"                                # Logging level (DEBUG, INFO, WARNING, ERROR)
  file: null                                   # Set to a path to log to a file

# Monitoring Configuration
# monitoring:
#   langfuse:
#     enabled: false                           # Set to true to enable Langfuse monitoring
#     host: "https://cloud.langfuse.com"       # Langfuse host URL
#     public_key: ""                           # Your Langfuse public key
#     secret_key: ""                           # Your Langfuse secret key

# Tools Configuration
tools:
  think:
    enabled: true
    url: "http://localhost:8000/sse"
    command: "uvx --from git+https://github.com/ddkang1/mcp-think mcp-think"
    transport: stdio_to_sse
  
  ddg_mcp:
    enabled: true
    url: "http://localhost:8001/sse"
    command: "uvx --from git+https://github.com/ddkang1/ddg-mcp ddg-mcp"
    transport: stdio_to_sse
  
  python_repl:
    enabled: true
    url: "http://localhost:8002/sse"
    command: "docker run -i --rm --pull=always -v ./data:/mnt/data/ ghcr.io/ddkang1/mcp-py-repl:latest"
    transport: stdio_to_sse
  
  # Remote tool example
  remote_tool:
    enabled: false
    url: "https://api.remote-tool.com/sse"
    transport: sse